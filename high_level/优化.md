

### cache友好

我们可以通过编程来充分利用cache性能，但是我们需要同时考虑数据的组织形式和数据的访问形式，此外，
可以使用分块处理的方式获得空间局部性更好的方法。获得最佳Cache性能是与平台相关的，比如，
Cache的大小，Cache行大小，以及映射策略。比较通用的法则当然是工作集越小越好，访问跨距越小越好。

一段Cache友好代码往往运行速度较快。但我们需要注意以下两点：
 - 尽可能多的重复使用一个数据（时间局限性）【如果我们需要在某个任务多次使用一个数据时，应该尽可能的一次性使用完，利用了数据的局部性特点】
 - 尽可能跨距为1的访问数据（空间局部性）【在访问一个数据时，应该依次的访问数组元素，不要跳着访问，利用了数据的空间局限性】

当我们访问一个内存地址单元时会将同一块的的数据同时从内存读入到Cache,这样如果我们继续访问附近的数据，那它就已经位于Cache中，访问速度就会很快。

对于矩阵乘法，可以通过调整ijk的顺序提高cache命中。并且可以使用分块的思想：L2 Cache大小有限，如果不停地访存很快会把L2 Cache刷满一遍。因此应该让高密度计算中的访存范围集中，使用分块的方法。

自己的总结：我们应该尽量让当前运算的部分优先完全放在register - l1 - l2 - l3 - memory中，并且下次用到的优先完全放在 l1 - l2 - l3 - memory。 并且可以利用SIMD的指令集（下发一个指令可以同时对多个数据做运算，这就是单指令多数据流）。

### 字节对齐、 alignas

字节对齐有以下准则：
 - 结构体变量的首地址能够被其对齐字节数大小所整除。
 - 结构体每个成员相对结构体首地址的偏移都是成员大小的整数倍，如不满足，对前一个成员填充字节以满足。
 - 结构体的总大小为结构体对最大成员大小的整数倍，如不满足，最后填充字节以满足。

alignas用来设置内存中对齐方式。比如可以把结构体的大小按64字节（cache line大小）对齐：
```cpp
struct alignas(64) FeatureValue {
...
};
```

### SIMD与SIMT

SIMD是指同一条指令多个数据。SIMT是同一条指令多个线程。

SIMD：采用一个控制器来控制多个处理单元，同时对一组数据的元素分别执行相同的操作从而实现空间上并行的技术。

SIMT：由一组标量处理单元构成，每个处理单元对应一个硬件线程，所有处理单元共享指令预取/译码模块并接收同一指令，运行其上的线程可以有自己的寄存器、独立的内存访问寻址、执行分支。


为了避免一些高延迟指令引起处理单元流水线停顿，CPU和GPU采取了完全不同的做法：
 - CPU的做法是一方面穷尽所能充分挖掘指令级并行来规避，另一方面通过`多级Cache、分支预测来掩盖访问内存延迟`(提高数据缓存命中率、指令缓存命中率)，万不得已CPU才会切换到别的硬件线程执行。硬件线程数量太多切换太频繁即使有助于整体吞吐却恶化单个线程的延迟对CPU设计来说也是不可接受的，所以我们可以看到Hyperthread的数目一般都比较少。

 - GPU的做法是另外一种思路，大规模数据并行带来`海量的可执行线程`，GPU完全可以`通过切换到别的线程Warp来规避指令延迟带来处理单元的停顿`。这种切换会非常频繁，需要在很短时间完成(比如一个时钟)，所以无论每个线程执行需要的的寄存器堆还是Block之内线程的Shared Memory从一开始就要分配妥当，`切换过程中线程上下文一直驻留，直到线程或者整个Block执行结束才能释放`。所以相比CPU，`GPU的Register非常多，而其处理单元的设计却可以异常简单`。

### TVM

受到编译器解决方法的启发，深度学习编译器被提出，我们可以将各个训练框架训练出来的模型看作各种编程语言，然后将这些模型传入深度学习编译器之后吐出IR，由于深度学习的IR其实就是计算图，所以可以直接叫作Graph IR。针对这些Graph IR可以做一些计算图优化再吐出Graph IR，最后对于不同的后端，Graph IR都会被编译为特定后端可以识别的代码完成模型推理。比如对于CPU，就吐出LLVM可以识别的IR，再通过LLVM编译器编译到CPU上执行。

TVM：深度学习编译器。

Pass是TVM中基于IR进行的一系列优化，它可以简化计算图，去除一些冗余的算子，提高模型的推理效率。

pass举例有：
 - RemoveUnusedFunctions：去除IR中的冗余节点。
 - ToBasicBlockNormalForm：将具有相同scope的节点放到一起，简化抽象语法树。（简化计算图）
 - EliminateCommonSubexpr：消除公共子表达式。（简化计算图）

### TensorRT

高性能的深度学习推理优化器。

不同的设备上存在不同的内存结构以及计算原语，传统的做法是在图层面做一些优化，并配合专家实现的在特定设备上的高性能算子完成任务。这当中比较知名的就是Nvidia TensorRT。

### XLA

Tensorflow内置的用于加速的编译器。

JIT是指，在Tensorflow运行时，无论是训练还是推理，从Tensorflow Graph中切割一部分子图交由XLA编译并运行。JIT的好处就是对用户的负担小，Tensorflow用户只要打开一个开关即可享受到加速的收益。

JIT的驱动方式是向Tensorflow注册了多个优化PASS，比如自动寻找子图、用户精细化地控制计算图中哪部分启用XLA等。

### LLVM

我们可以将自己语言的源代码编译成LLVM中间代码（LLVM IR），然后由LLVM自己的后端对这个中间代码进行优化，并且编译到相应的平台的二进制程序。

### GPU redution 

[reduction](https://developer.download.nvidia.cn/compute/cuda/1.1-Beta/x86_website/projects/reduction/doc/reduction.pdf)

解决问题：
 - Warp Divergence： 减少分支语句，或者把if分支中线程弄成连续的。
 - 从Global Memory到SM的数据传输是Cache Line的方式（比如128字节）。使同一个时刻，不同线程访问连续地址，提高cache命中率。
 - Shared Memory Bank Conflicts：假定申请了100个4字节宽度的int数据的shared memory空间，那么第0\~31（按照32banks为例）个数据会依次存放于bank0\~bank31,第32\~63个数据会存在bank0\~bank31的下一行，以此类推。也就是说避免多个线程对同一个bank同时访问（访问同一个地址例外，因为可以broadcast）。
 - unroll loops：展开最后一个warp（减少一次\_\_syncthreads）、展开循环(减少指令)。（volatile避免中间结果被缓存到寄存器里面，而是每次去从GPU的shared memory里面去读取）
 - 其他：避免出现较多的线程空闲。

Unrolling 在CUDA编程中意义更重。我们的目标依然是通过减少指令执行消耗，增加更多的独立指令来提高性能。这样就会增加更多的并行操作从而产生更高的指令和内存带宽（bandwidth）。也就提供了更多的eligible warps来帮助hide instruction/memory latency 。

### GPU prefix sum

 - 避免过多的分支（zero padding）
 - 避免bank conflict
 - warp shuffle：只要两个thread在同一个warp中，这种比通过shared Memory进行thread间的通讯效果更好，latency更低，同时也不消耗额外的内存资源来执行数据交换
