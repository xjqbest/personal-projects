

## paddle fleet

#### (1) 数据量大
 
 - 样本数据结构优化。每个slot对应三个vector 改为 每个样本存slot、key的pair。
 
 - shuffle：如何充分shuffle数据（worker本地shuffle + worker间通过rpc shuffle），不丢数据（发数据做流控、收数据增大线程数），数据分布

 - dataset InMemory模式 和 Queue模式，内存装得下就用前者，否则就用后者。

#### (2) 参数量大

 - hash map性能：通常影响它的几个主要的因素是`哈希函数的选择`（尽量减少哈希碰撞）、`哈希冲突`、`内存分配`。哈希冲突的解决可以采用cache友好的线性探测法。内存分配可以采用事先一个内存池作为allocator或者使用tcmalloc或者jemalloc等库。针对不同的场景，可以采用不同的hash表的实现。

 - server端sparse参数分多个shard，由多个线程分别处理，提高并发。

 - pull对key去重、push merge多个batch梯度，缓解热点key的问题。

 - dense参数原本是每个线程独立一份，并且每个batch都pull/push一次，改为多线程共享一份dense参数，并且是单独的异步更新，因为数据量大，对效果影响很小。
 
 - 分层参数服务器（HBM-MEM-SSD），做单机GPU训练。

 
不同的哈希表：
 - dense_hash_map，占用内存较多,而且内存一大块一大块申请的。适合内存充足的情况。二次内部探测处理碰撞，更加cache友好。（unordered_map采用链表的方法解决碰撞）
 - sparse_hash_map，占用的内存较少，适合内存比较紧张的情况。它分成了若干group，对元素的操作都在group内部完成，
 - 还有一些特殊实现的哈希表，比如Cuckoo Hash（布谷鸟哈希），它维护了两个hash table，最大的特点是lookup最坏时间复杂度O(1)。还有Robin Hood hash（罗宾汉哈希）也具有cache friendly的特点，查找不存在的key也能很快。


#### 通用性、易用性

 - embedding layer能够更灵活组网：将pull op化。

 - 强化学习DQN，target网络更新：copy table。

 - 自定义metric计算：提供一些基本操作的类（global sum/mean/avg）或者op。

 - 配置：避免比较多的set_xxx，将配置作为一整个字典传入。



## GPU Box

目标是用单机GPU训练原先需要几十个cpu节点训练的模型，也就是借助GPU的更强的算力、更高的性价比，达到节省成本的目的。

 - 单机的显存和内存通常有限，如何存的下大模型：将参数存到SSD这种容量的存储上，问题是SSD IO的延迟至少比 MEM 低了3个左右的数量级，因此采用分层参数服务器 SSD-MEM-HBM，掩盖SSD参数的IO延迟。也就是说mem是ssd的缓存，hbm是mem的缓存。

 - 问题又来了，随着训练时间推移，HBM中的key会越来越多，虽然可能有特征淘汰策略，但依然可能出现显存或者内存不够的情况。并且更关键的一点是，训练过程中，依然会有内存-显存之间的数据和参数拷贝，会有额外的开销，为了让训练过程中完全在gpu中进行，不与cpu交互，可以采用大batch流水线的方法，预先把大batch的所有参数拷贝到显存的哈希表中，也可以看出这个哈希表其实大小是固定的，不会出现rehash。训练完这个大batch后，再把参数拷贝回mem，进而再把低频key拷贝回ssd。

SSD
 - 直接记录key到ssd file的映射，内存通常装不下，因此将key分group，记录key到group的映射。
 - 为了避免SSD频繁做不必要的读写，利用布隆过滤器先判断key存不存在。
 - 参数dump回ssd时，首先找到它们对应的ssd file，首先写入到新文件，然后后台有个compation线程异步与老文件做merge。

## hippo

同步训练中的数据分配：每个minibatch被分成若干block分配给所有worker，如何尽量提高训练速度。
 - （1）慢节点少分配数据，正常节点多分配数据
 - （2）由于慢节点通常是随机的出现一段时间，可能后面就成为了正常节点，避免它由于中间一段时间成为慢节点后就一直饥饿。
  
因此给每个worker一个权重，同时考虑它的消费block的速度和等待数据的时间。分配时不能简单按照权重成反比来分配，而是采用一个最小堆，贪心的求总时间最短。



## numerous

相对而言，目前支持的业务参数量不大。

数据遇到过磁盘IO瓶颈。并且将原始数据的预处理（特征id化、特征交叉）提前。这两点解决，基本就满足了cpu分布式训练。


## hugectr

hugectr是nvidia开发的支持大规模稀疏参数的gpu训练框架。embedding存储有两种模式，local模式和distributed模式。前者适合小模型，后者适合大模型。

local模式：本地做完lookup，可以做sum/mean/concat操作，接着通过scatter将参数发到
