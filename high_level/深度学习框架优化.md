
## 训练优化

![image](https://user-images.githubusercontent.com/12492564/150645777-1206de3c-f7c3-4156-bba8-fe863e649179.png)


![image](https://user-images.githubusercontent.com/12492564/150645806-f0c60cf9-9703-4fef-9558-2d1be18ab44e.png)

## 推理优化

推断（Inference）和训练（Training）的不同：
 - 模型固定，可以对计算图进行优化
 - 输入输出大小固定，可以做memory优化
 - 推断（Inference）的batch size要小很多，吞吐降低，没有办法很好地利用GPU
 - 推断（Inference）可以使用低精度的技术，训练的时候因为要保证前后向传播，每次梯度的更新是很微小的，这个时候需要相对较高的精度

优化：
 - tensorrt：将tf等框架的计算图转为tensorrt中，然后在tensorRT中可以针对NVIDIA自家GPU实施优化策略（算子融合、量化、显存复用、多stream并行）
 - xla：tensorflow内置的编译优化工具（算子的融合）
 - tvm：更全面。将不同前端深度学习框架训练的模型，转换为统一的中间语言表示，并对其进行优化，转换为目标硬件上的代码逻辑。

## 其他细节

float精度丢失：在加法运算过程中，指数位较小的数，需要在有效位进行右移（先对齐再计算），在右移的过程中，最右侧的有效位就被丢弃掉了。这会导致对应的指数位较小的数，在加法发生之前，就丢失精度。

NUMA：多个cpu都通过一个总线访问内存，就会出现瓶颈，因此有了NUMA。NUMA引入了本地内存和远程内存，CPU 访问本地内存的延迟会小于访问远程内存；NUMA的内存分配与内存回收策略结合时会可能会导致 Linux 的频繁交换分区（Swap）进而影响系统的稳定性；

## 其他优化

IO优化：worker训练不稳定，一段时间会成为慢节点；一台机器分配较多worker时，这些worker都成为慢节点。慢节点的磁盘IO使用率高，cpu利用率低。download，parse，train三个阶段，downoad耗时最高。也就是说磁盘IO是瓶颈。为了解决这个问题，采用pipe的方式，数据不再落盘。并且更省内存（tf dataset prefetch需要预取数据，pipe本身就是流式），更灵活（pipe可以灵活自定义解析）。

流式训练：解决样本落hdfs带来的延迟，使得模型具有更好的实时性。（提高kafka吞吐、并发，数据局部shuffle，保存时间戳和partition offset）。其他优化点：（1）实时的样本是通过flink进行日志流、特征流的join，那么就会有正负样本跨时间窗口的问题。（2）训练效果长期与基线持平很难，流式模型定期从离线训练模型热启动。

其他零碎一点的：指标上报的内存、embedding输出（w * x）从稠密存储改为稀疏、预测dump优化。
