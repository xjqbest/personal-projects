
## 训练优化

![image](https://user-images.githubusercontent.com/12492564/150645777-1206de3c-f7c3-4156-bba8-fe863e649179.png)


![image](https://user-images.githubusercontent.com/12492564/150645806-f0c60cf9-9703-4fef-9558-2d1be18ab44e.png)

## 推理优化

推断（Inference）和训练（Training）的不同：
 - 模型固定，可以对计算图进行优化
 - 输入输出大小固定，可以做memory优化
 - 推断（Inference）的batch size要小很多，吞吐降低，没有办法很好地利用GPU
 - 推断（Inference）可以使用低精度的技术，训练的时候因为要保证前后向传播，每次梯度的更新是很微小的，这个时候需要相对较高的精度

优化：
 - tensorrt：将tf等框架的计算图转为tensorrt中，然后在tensorRT中可以针对NVIDIA自家GPU实施优化策略（算子融合、量化、显存复用、多stream并行）
 - xla：tensorflow内置的编译优化工具（算子的融合）
 - tvm：更全面。将不同前端深度学习框架训练的模型，转换为统一的中间语言表示，并对其进行优化，转换为目标硬件上的代码逻辑。

## 其他细节

`float精度丢失`：在加法运算过程中，指数位较小的数，需要在有效位进行右移（先对齐再计算），在右移的过程中，最右侧的有效位就被丢弃掉了。这会导致对应的指数位较小的数，在加法发生之前，就丢失精度。

`NUMA`：多个cpu都通过一个总线访问内存，就会出现瓶颈，因此有了NUMA。NUMA引入了本地内存和远程内存，CPU 访问本地内存的延迟会小于访问远程内存；NUMA的内存分配与内存回收策略结合时会可能会导致 Linux 的频繁交换分区（Swap）进而影响系统的稳定性；

## 其他优化

`IO优化`：worker训练不稳定，一段时间会成为慢节点；一台机器分配较多worker时，这些worker都成为慢节点。慢节点的磁盘IO使用率高，cpu利用率低。download，parse，train三个阶段，downoad耗时最高。也就是说磁盘IO是瓶颈。为了解决这个问题，采用pipe的方式，数据不再落盘。并且更省内存（tf dataset prefetch需要预取数据，pipe本身就是流式），更灵活（pipe可以灵活自定义解析）。

`流式训练`：解决样本落hdfs带来的延迟，使得模型具有更好的实时性。（提高kafka吞吐、并发，数据局部shuffle，保存时间戳和partition offset）。其他优化点：（1）实时的样本是通过flink进行日志流、特征流的join，那么就会有正负样本跨时间窗口的问题。（2）训练效果长期与基线持平很难，流式模型定期从离线训练模型热启动。

其他零碎一点的：指标上报的内存、embedding输出（w * x）从稠密存储改为稀疏、预测dump优化。

## 补充

`ringallreduce`： 将每个gpu上的数组分成N份（gpu个数），首先执行（N-1）次scatter-reduce，每个gpu每次向其右邻居发送一份数据。做完后每个gpu上都有一份（数组的1/N）完整的数据，然后接着再做（N-1）次all-gather，也是同样的过程（区别就是向其右邻居发送一份数据后是直接覆盖）。总的传输量就是 2 * （N-1） * K / N，其中K为数组占的内存大小。

`recompute（Forward Recomputation Backpropagation）`：思想是将深度学习网络切分为k个部分（segments）。对每个segment而言：前向计算时，除了小部分必须存储在内存中的Variable外，其他中间结果都将被删除；在反向计算中，首先重新计算一遍前向算子，以获得中间结果，再运行反向算子。简而言之，FRB和普通的网络迭代相比，多计算了一遍前向算子。我们把切分网络的变量叫做checkpoints。如果checkpoints很少， 那么Recompute起的作用有限；如果checkpoints数量过多， 那么checkpoints本身占用的内存量就较大，内存消耗可能不降反升。

`DGC（Deep Gradient Compression）`：分布式SGD中有99.9%的梯度交换都是冗余的，可以使用深度梯度压缩选择重要梯度进行通信来减少通信量，降低对通信带宽的依赖。基本思路是通过只传送重要梯度，即只发送大于给定阈值的梯度来减少通信带宽的使用。为避免信息的丢失，DGC会将剩余梯度在局部累加起来，最终这些梯度会累加大到足以传输。如何避免稀疏梯度通信训练带来的最终模型精度损失：预热训练（前几轮正常通信，后面逐步提升稀疏度），过滤掉过于陈旧的梯度。

`local sgd`：使用数据并行训练时，会遇到（1）慢节点和通信延迟（2）增大了实际的batch size，影响训练精度。local sgd通过延长节点间同步的间隔(局部异步训练)来减轻慢节点的影响和减少通信频率，以此提升训练的吞吐。在Local SGD训练中，集群中的每个训练进程各自会独立的进行H个连续的SGD更新，然后集群中的所有训练进程会进行通信，同步（averaging）所有训练进程上的参数。如何确定参数同步的间隔(频率)，以达到训练吞吐和训练精度间更好的平衡：第一阶段训练进程间同步的间隔为1个步长，即同步SGD，来保证最终训练精度；在第二阶段增大同步间隔到固定常数H，来提升训练吞吐。


