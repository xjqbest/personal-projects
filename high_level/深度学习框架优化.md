
## 训练优化

![image](https://user-images.githubusercontent.com/12492564/150645777-1206de3c-f7c3-4156-bba8-fe863e649179.png)


![image](https://user-images.githubusercontent.com/12492564/150645806-f0c60cf9-9703-4fef-9558-2d1be18ab44e.png)

## 推理优化

推断（Inference）和训练（Training）的不同：
 - 模型固定，可以对计算图进行优化
 - 输入输出大小固定，可以做memory优化
 - 推断（Inference）的batch size要小很多，吞吐降低，没有办法很好地利用GPU
 - 推断（Inference）可以使用低精度的技术，训练的时候因为要保证前后向传播，每次梯度的更新是很微小的，这个时候需要相对较高的精度

优化：
 - tensorrt：将tf等框架的计算图转为tensorrt中，然后在tensorRT中可以针对NVIDIA自家GPU实施优化策略（算子融合、量化、显存复用、多stream并行）
 - xla：tensorflow内置的编译优化工具（算子的融合）
 - tvm：更全面。将不同前端深度学习框架训练的模型，转换为统一的中间语言表示，并对其进行优化，转换为目标硬件上的代码逻辑。

## 其他细节

float精度丢失：在加法运算过程中，指数位较小的数，需要在有效位进行右移（先对齐再计算），在右移的过程中，最右侧的有效位就被丢弃掉了。这会导致对应的指数位较小的数，在加法发生之前，就丢失精度。

NUMA：多个cpu都通过一个总线访问内存，就会出现瓶颈，因此有了NUMA。NUMA引入了本地内存和远程内存，CPU 访问本地内存的延迟会小于访问远程内存；NUMA的内存分配与内存回收策略结合时会可能会导致 Linux 的频繁交换分区（Swap）进而影响系统的稳定性；
