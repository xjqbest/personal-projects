
### 五个基本元素

`agent`在`状态s`选择一个`动作a`，`环境`给agent反馈一个`reward` ，同时 agent 进入一个新的状态。

### 几个组成部分

对于一个强化学习 agent，它可能有一个或多个如下的组成成分：
 - `策略函数(policy function)`：agent 会用这个函数来选取下一步的动作。（它其实是一个函数，把输入的状态变成行为）。
 - `价值函数(value function)`：价值函数是未来奖励的一个预测，用来评估状态的好坏。（价值函数里面有一个`折扣因子`，来区分当前回报和未来回报的重要性。）
 - `模型(model)`：状态转移概率 + 奖励函数。（下一步的状态取决于你当前的状态以及你当前采取的行为）。

当我们有了这三个组成部分过后，就形成了一个 `马尔可夫决策过程`。

### agent的类型

#### 根据 agent 学习的东西不同

 - `基于价值的 agent(value-based）`：agent显式地学习的是价值函数。策略是从学到的价值函数里面推算出来的。
 - `基于策略的 agent（policy-based`：显式学习policy（给它一个状态，它就会输出动作的概率），
 - 把 value-based 和 policy-based 结合起来就有了 `Actor-Critic` agent：这一类 agent 把它的策略函数和价值函数都学习了，然后通过两者的交互得到一个最佳的行为。

在基于`策略迭代`的强化学习方法中，agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。

在基于`价值迭代`的强化学习方法中，agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下，
对于行为集合规模庞大、动作连续的场景，其很难学习到较好的结果。

#### 根据 agent 有没有学习环境模型

- `model-based(有模型)`：通过学习状态的转移来采取动作。
- `model-free(无模型)`：通过学习价值函数和策略函数进行决策。

当agent知道状态转移函数 P 和奖励函数 R 后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样agent就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。
这种学习方法称为有模型学习。

在实际应用中，agent并不是那么容易就能知晓环境中的所有元素的。通常情况下，状态转移函数和奖励函数很难估计，这时就需要采用无模型学习。
没有对真实环境进行建模，agent只能在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新行为策略，这样反复迭代直到学习到最优策略。


在没法获取MDP的模型（无模型）情况下，我们可以通过以下两种方法来估计某个给定策略的价值：
 - `蒙特卡罗`：让 agent 跟环境进行交互，就会得到很多轨迹。每个轨迹都有对应的 reward，每个轨迹的 reward 进行平均，就可以知道某一个策略下面的状态的价值。
 - `时序差分`：每往前走一步，就用得到的估计回报来更新上一步值。

在没法获取MDP的模型（无模型）情况下，如何优化价值函数，得到最佳的策略：
 - 1、根据给定的当前的 policy  来估计价值函数。
 - 2、得到估计的价值函数后，通过 greedy 的方法来改进它的算法。

### on-policy vs off-policy

off-policy 在学习的过程中，同时有两种策略：
 - `target policy(目标策略)`：可以根据自己的经验来学习最优的策略，不需要去和环境交互
 - `behavior policy(行为策略)`：探索到所有可能的轨迹，采集轨迹，采集数据，然后把采集到的数据喂给 target policy 去学习。（target policy优化的时候，不会管实际下一步去往哪里探索，它就只选收益最大的策略，也可以说更激进。）

off-policy好处：
 - 学习其他 agent 的行为，模仿学习。
 - 重用老的策略产生的轨迹。

![image](https://user-images.githubusercontent.com/12492564/150631985-6dea2eb0-910c-4f1e-bc84-093de072fe70.png)


### policy gradient

![image](https://user-images.githubusercontent.com/12492564/150632334-f46f68f6-e17b-4ffe-8298-ee615f7e6baa.png)

首先我们需要一个 `policy model` 来输出动作概率，输出动作概率后，我们 `sample()` 函数去得到一个具体的动作，然后跟环境交互过后，我们可以得到一整个回合的数据。拿到回合数据之后，我再去执行一下 `learn()` 函数，在 `learn()` 函数里面，我就可以拿这些数据去构造损失函数，扔给这个优化器去优化，去更新我的 `policy model`。

我们可以把它想成一个分类的问题，在分类里面就是输入一个图像，然后输出决定说是 10 个类里面的哪一个。在做分类时，我们要收集一堆训练数据，要有输入跟输出的对。

在实现的时候，你就把状态当作是分类器的输入。 你就当在做图像分类的问题，只是现在的类不是说图像里面有什么东西，而是说看到这张图像我们要采取什么样的行为，每一个行为就是一个类。比如说第一个类叫做向左，第二个类叫做向右，第三个类叫做开火。

在做分类的问题时，要有输入和正确的输出，要有训练数据。而这些训练数据是从采样的过程来的。假设在采样的过程里面，在某一个状态，你采样到你要采取动作 a， 你就把这个动作 a 当作是你的 ground truth。你在这个状态，你采样到要向左。 本来向左这件事概率不一定是最高， 因为你是采样，它不一定概率最高。假设你采样到向左，在训练的时候，你告诉机器说，调整网络的参数， 如果看到这个状态，你就向左。

RL 唯一不同的地方是 loss 前面乘上一个权重：整场游戏得到的总奖励 R，它并不是在状态 s 采取动作 a 的时候得到的奖励。


#### policy gradient 如何从on-policy转换为off-policy

原来的旧数据是在旧的policy下采样出来的，一旦更新policy，旧数据就不能用了。

之前我们是拿 `policy 1` 去跟环境做互动，采样出轨迹，然后计算梯度。现在我们不用 `policy 1` 去跟环境做互动，假设有另外一个 `policy 2` 
它去跟环境做互动(做示范)，告诉`policy 1` 说，它跟环境做互动会发生什么事，借此来训练`policy 1`。

补上重要性权重即可。
