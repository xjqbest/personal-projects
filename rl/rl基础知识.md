
### 五个基本元素

`agent`在`状态s`选择一个`动作a`，`环境`给agent反馈一个`reward` ，同时 agent 进入一个新的状态。

### 几个组成部分

对于一个强化学习 agent，它可能有一个或多个如下的组成成分：
 - `策略函数(policy function)`：agent 会用这个函数来选取下一步的动作。（它其实是一个函数，把输入的状态变成行为）。
 - `价值函数(value function)`：价值函数是未来奖励的一个预测，用来评估状态的好坏。（价值函数里面有一个`折扣因子`，来区分当前回报和未来回报的重要性。）
 - `模型(model)`：状态转移概率 + 奖励函数。（下一步的状态取决于你当前的状态以及你当前采取的行为）。

当我们有了这三个组成部分过后，就形成了一个 `马尔可夫决策过程`。

### agent的类型

#### 根据 agent 学习的东西不同

 - `基于价值的 agent(value-based）`：agent显式地学习的是价值函数。策略是从学到的价值函数里面推算出来的。
 - `基于策略的 agent（policy-based`：显式学习policy（给它一个状态，它就会输出动作的概率），
 - 把 value-based 和 policy-based 结合起来就有了 `Actor-Critic` agent：这一类 agent 把它的策略函数和价值函数都学习了，然后通过两者的交互得到一个最佳的行为。

在基于`策略迭代`的强化学习方法中，agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。

在基于`价值迭代`的强化学习方法中，agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下，
对于行为集合规模庞大、动作连续的场景，其很难学习到较好的结果。

#### 根据 agent 有没有学习环境模型

- model-based(有模型)：通过学习状态的转移来采取动作。
- model-free(无模型)：通过学习价值函数和策略函数进行决策。

当agent知道状态转移函数 P 和奖励函数 R 后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样agent就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。
这种学习方法称为有模型学习。

在实际应用中，agent并不是那么容易就能知晓环境中的所有元素的。通常情况下，状态转移函数和奖励函数很难估计，这时就需要采用无模型学习。
没有对真实环境进行建模，agent只能在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新行为策略，这样反复迭代直到学习到最优策略。
