
### DDPG

DQN用神经网络解决了Q-learning不能解决的连续状态空间问题。（也就是说DQN能够解决状态无限，动作有限的问题）

![image](https://user-images.githubusercontent.com/12492564/152301730-6cc7c58f-026a-4b1d-ae43-ac2fe6af47b7.png)


DQN不能用于连续动作问题原因，是因为maxQ(s',a')函数只能处理离散型的，因此采用一个“函数”，直接替代maxQ(s',a')的功能。也就是说，我们期待我们输入状态s，
这个“函数”返回我们动作action的取值，这个取值能够让q值最大。这个就是DDPG中的Actor的功能。

![image](https://user-images.githubusercontent.com/12492564/152303401-8d4e2fce-077e-4c28-8cf7-0231d2749e0a.png)

![image](https://user-images.githubusercontent.com/12492564/152568201-5b95efc7-0aac-46dc-891f-5878446a0f98.png)

和DQN一样，更新的时候如果更新目标在不断变动，会造成更新困难。所以DDPG和DQN一样，用了target网络。

### AMC(AutoML model compression)

使用layer-wise的方式来处理网络，对于每一层输出一个压缩比，然后移到下一层。


### 通道裁剪

cnn的计算量主要在卷积层，通过裁剪掉卷积层的channel和它对应的kernel，并最小化裁剪channel后与原始输出的误差。

分两步：
 - 选择哪些channel被裁剪，求出每个channel的权重，如果为0即是被裁减。
 - 重建输出，直接使用最小平方误差来拟合原始卷积层的输出。
